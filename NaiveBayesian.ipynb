{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navie Bayesian\n",
    "\n",
    "* 가능도likelihood를 계산할 경우, 속성 간의 관계성을 무시하고 계산.\n",
    "* Why Naive? independence assumption, otherwise very big data samples are needed\n",
    "* 계산의 복잡성을 줄이기 위한 가정.\n",
    "* 예, 속성이 2가지 값을 갖고, 이러한 속성이 5개 있다면 $2^5=512$ 경우의 확률을 계산해야 하지만\n",
    "NB의 경우, $2 \\times 5 = 10$개를 계산.\n",
    "\n",
    "## Probability\n",
    "\n",
    "* frequentists\n",
    "    * 확률이란 오랫 동안 반복하면 기대하는 빈도, 즉 P(A) = n/N, n은 N회 반복했을 경우 A의 기대 값\n",
    "    * 주사위를 던지는 경우, 빈도에 따라 1/6로 확률이 정해짐.\n",
    "    * 무작위로, 충분히 샘플링을 하여 발생한 과거의 데이터로 계산.\n",
    "    * 1회 던지는 경우, 1/6이라는 정확한 확률이 나오지 않을 수 있슴.\n",
    "    * 사전 지식이 무의미 함.\n",
    "    * 통계적 추정을 할 경우, 그 값이 정해져 있다고 봄. 평균 값의 예, 신뢰구간을 사용하면 그 구간에 반드시 그 값 존재.\n",
    "\n",
    "* Bayesian\n",
    "    * 확률이란 불확실한 사건에 대한 자신이 믿고 있는 정도.\n",
    "    * 내기betting (야구) - 정보가 부족하므로, 정보가 추가적으로 주어지면 갱신.\n",
    "    * 1회 던지는 경우에도 현재의 주관적 믿음에 따라 확률을 정하고, 다음 증거에 따라 갱신됨.\n",
    "    * 따라서 1) 사전확률prior을 정하고, 2) 주어지는 증거에 따라 갱신likelihood, 3) 사후확률posterior 추정\n",
    "    * 평균 값을 추정을 할 경우, 그 값은 불확실하며 분포에 따라 신뢰구간 내에 존재할 수도 있고 없을 수도 있슴.\n",
    "\n",
    "* 사전확률\n",
    "    * 전혀 모를 경우 uniform distribution\n",
    "    * 켤레 사전확률 conjugate prior\n",
    "        * introduced in Raiffa and Schlaifer (1961)\n",
    "        * likelihood가 특정 분포를 따른다고 가정할 경우, 사전확률과 사후확률은 동일한 분포를 따름\n",
    "            * normal - normal\n",
    "            * poisson - Gamma\n",
    "            * binomial - Beta\n",
    "            * multinomial - Dirichlet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Probability\n",
    "\n",
    "* 문제를 관련 속성의 값에 따라 그 값을 결정하는 경우\n",
    "* 속성의 집합 feature vector x = (x1,...,xn), 그리고 Ck (k개의 결과)\n",
    "* Naive 가정 - 속성 간에는 서로 관련성이 없는 독립성.\n",
    "* 결과는 최대의 확률을 가진 경우로 결정\n",
    "* 확률은 $p(C_k | x_1,\\ldots,x_n)$\n",
    "* 아래의 예: Ck(t,f $\\in$ C), x1(m,g,h $\\in$ A), x2(b,s,q $\\in$ B)\n",
    "* 위 식을 베이지안 확률로 나타내면:\n",
    "\n",
    "$posterior = \\frac{prior \\times likelihood}{evidence}$\n",
    "\n",
    "$p(C_k \\vert x) = \\frac{p(C_k) \\ p(x \\vert C_k)}{p(x)}$\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "p(C_k \\vert x_1, \\dots, x_n)\n",
    "    & \\varpropto p(C_k, x_1, \\dots, x_n) \\\\\n",
    "    & \\varpropto p(C_k) p(x_1 \\vert C_k) \\\n",
    "         p(x_2\\vert C_k) p(x_3\\vert C_k) \\cdots \\\\\n",
    "    & \\varpropto p(C_k) \\prod_{i=1}^n p(x_i \\vert C_k)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "* 위 식에서 독립성 가정을 넣어, 순수베이지안을 Naive로 변경\n",
    "\n",
    "## Laplace smooting\n",
    "\n",
    "* 새로운 경우 발생할 수 있는 0값을 막기 위해 아주 적은 수를 더하여 줌."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian 확률\n",
    "\n",
    "문제 1: 일반적인 확률\n",
    "* 7개의 공이 있다. 이 가운데 하얀 공 3 (W W W), 검은 공 4개 (B B B B).\n",
    "* 이런 경우, p(W) = 3/7, p(B) = 4/7\n",
    "\n",
    "문제 2: 조건부 확률\n",
    "\n",
    "* 2개의 주머니가 있는 경우 (B1 B2)\n",
    "* B1 - 흰공W 2개 검은공 2개\n",
    "* B2 - 흰공W 1개 검은공 2개\n",
    "* $P(A=B2|B=W) =\\frac{P(B2\\ and\\ W)}{P(W)} = \\frac{1/7}{3/7}=1/3$\n",
    "\n",
    "문제 3: 베이지안\n",
    "\n",
    "* 위에서 $P(A=B2|B=W) = \\frac{P(B2\\ and\\ W)}{P(W)}$\n",
    "* 주머니B2를 선택해서 공을 하나 뽑을 경우 흰 공일 확률로 바꾸어 쓰면:\n",
    "    * $P(B=W|A=B2) = \\frac{P(W\\ and\\ B2)}{P(B2)}$\n",
    "    * $P(W|B2) \\times P(B2) = P(W\\ and\\ B2)$\n",
    "* 위 식에 대입하면:\n",
    "    * $P(A=B2|B=W) = \\frac{P(W|B2) P(B2)}{P(W)}$\n",
    "\n",
    "|    | W | B |\n",
    "|----|---|---|\n",
    "| B1 | 2 | 2 |\n",
    "| B2 | 1 | 2 |\n",
    "\n",
    "* 흰공이 나왔는데, 가방2에서 나왔을 확률\n",
    "* $P(A=B2|B=W)=\\frac{P(W|B2) P(B2)}{P(W)} = \\frac{(1/3)(3/7)}{(3/7)}=1/3$\n",
    "\n",
    "|     | 강우 | 맑음 |\n",
    "|-----|-----|-----|\n",
    "| 승리 |  3  |  2  |\n",
    "| 패배 |  1  |  6  |\n",
    "\n",
    "* 오늘 비가 오고 있는데, 이 경우 승리할 확률을 구하면,\n",
    "* $P(A=승리|B=비) = \\frac{P(B|A) P(A)}{P(B)} = \\frac{(3/5) (5/12)}{(4/12)} = 3/4 =0.75$\n",
    "* 즉, 사전확률prior probability 5/12, 가능도likelihood 3/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bayesian Inference\n",
    "\n",
    "* ML (Maximum likelihood estimation)\n",
    "    * $Y_{ML}=argmax_Y P(X | Y)$\n",
    "* MAP (Maximum a posteriori estimation)\n",
    "    * $Y_{MAP}=argmax_Y P(Y | X)$\n",
    "    * C=$argmax_{c_i \\in C} P(c_i|x,y)$\n",
    "        * P(c1|x,y) If P(c1|x, y) > P(c2|x, y), the class is c1.\n",
    "        * P(c2|x,y) If P(c1|x, y) < P(c2|x, y), the class is c2.\n",
    "\n",
    "MAP를 풀어서 쓰면:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "Y_{MAP} &=argmax_Y P(X|Y)P(Y)\\\\\n",
    "Y_{MAP} &=argmax_Y P(X|Y)P(Y)\\\\\n",
    "Y_{MAP} &=argmax_Y (log P(X|Y) + log P(Y))\n",
    "\\end{align}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navie Bayesian algorithm\n",
    "\n",
    "* input: Training Data, Attribute List\n",
    "  * 학습데이터\n",
    "  * 속성목록\n",
    "* output: decision\n",
    "* 순서\n",
    "    * 사전확률prior\n",
    "    * 가능도likelihood 갱신\n",
    "    * deciison\n",
    "\n",
    "알고리듬에 따른 개발\n",
    "1. data read - 데이터 준비\n",
    "2. Prior\n",
    "3. Update\n",
    "4. decision - argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제\n",
    "\n",
    "* 비연속적 속성\n",
    "* source: from Bing Liu\n",
    "\n",
    "$Pr(C=c_j | d)$\n",
    "d가 주어졌을 때 C가 $c_j$일 확률\n",
    "\n",
    "문제: Naive Bayesian을 구해보면 (A=m, B=q, C=?)\n",
    "\n",
    "* C가 t인 확률: \n",
    "    * $Pr(C=t) \\times \\prod_{j=1}^2\\ Pr(A_j=a_j|C=t)$\n",
    "    * Pr(C=t) x ( Pr(A=m | C=t) x Pr(B=q | C=t) ) = 1/2 x 2/5 x 2/5 = 2/25\n",
    "* C가 f인 확률:\n",
    "    Pr(C=f) x ( Pr(A=m | C=f) x Pr(B=q | C=f) ) = 1/2 x 1/5 x 2/5 = 1/25\n",
    "* argmax(2/25,1/25)를 구하면 C=t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas\n",
      "   A  B  C\n",
      "0  m  b  t\n",
      "1  m  s  t\n",
      "2  g  q  t\n",
      "3  h  s  t\n",
      "4  g  q  t\n",
      "5  g  q  f\n",
      "6  g  s  f\n",
      "7  h  b  f\n",
      "8  h  q  f\n",
      "9  m  b  f\n",
      "\n",
      "[10 rows x 3 columns]\n",
      "['m' 'm' 'g' 'h' 'g' 'g' 'g' 'h' 'h' 'm']\n",
      "['s' 'q' 's' 'q']\n",
      "0.5\n",
      "{'h': 0.2, 'm': 0.4, 'g': 0.4}\n",
      "{'q': 0.4, 's': 0.4, 'b': 0.2}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "#1. data read\n",
    "#1.1 numpy data read\n",
    "x=np.array(\n",
    "[('m', 'b', 't'),\n",
    " ('m', 's', 't'),\n",
    " ('g', 'q', 't'),\n",
    " ('h', 's', 't'),\n",
    " ('g', 'q', 't'),\n",
    " ('g', 'q', 'f'),\n",
    " ('g', 's', 'f'),\n",
    " ('h', 'b', 'f'),\n",
    " ('h', 'q', 'f'),\n",
    " ('m', 'b', 'f')],\n",
    " dtype=[('A','a1'),('B','a1'),('C','a1')])\n",
    "\n",
    "# 1.2 numpy data read with a dtype seperately declared\n",
    "dt=np.dtype( {'names': ['A','B','C'], 'formats':['a1','a1','a1'] } )\n",
    "x=np.array(\n",
    "   [('m', 'b', 't'),\n",
    "    ('m', 's', 't'),\n",
    "    ('g', 'q', 't'),\n",
    "    ('h', 's', 't'),\n",
    "    ('g', 'q', 't'),\n",
    "    ('g', 'q', 'f'),\n",
    "    ('g', 's', 'f'),\n",
    "    ('h', 'b', 'f'),\n",
    "    ('h', 'q', 'f'),\n",
    "    ('m', 'b', 'f')],dtype=dt)\n",
    "\n",
    "# 1.3 pandas data read\n",
    "data=pd.DataFrame(x)\n",
    "print \"pandas\\n\",data\n",
    "\n",
    "#record array indexing\n",
    "print x['A']\n",
    "print x['B'][1:5]\n",
    "\n",
    "# 확률 구하는 간편한 함수 구현\n",
    "# 빈도 구하는 다른 방법 prior = collections.Counter(x['C'])\n",
    "prob=lambda x: dict((i,x.count(i)/float(len(x))) for i in set(x))\n",
    "\n",
    "# 2. prior\n",
    "c=x['C'].tolist()\n",
    "prior = prob(c)\n",
    "print prior['t']\n",
    "\n",
    "# 3. likelihood update\n",
    "# 분할해서 확률 (예: Prob(A=m|C=t)\n",
    "# C열 값이 true인 경우 A열 분할\n",
    "a=x[x['C']=='t']['A']\n",
    "a_likelihood=prob(a.tolist())\n",
    "print a_likelihood\n",
    "\n",
    "b=x[x['C']=='t']['B']\n",
    "b_likelihood=prob(b.tolist())\n",
    "print b_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제\n",
    "\n",
    "* 키, 몸무게와 같은 연속적 변수. 분포distribution에 따라 무작위로 발생.\n",
    "\n",
    "* 문제: height=6ft, weight=130lbs, foot size=8inches 경우 성별은?\n",
    "* source: http://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "\n",
    "| sex\t | height(feet) | weight(lbs) | foot size(inches) |\n",
    "|--------|-----:|----:|---:|\n",
    "| male   | 6    | 180 | 12 |\n",
    "| male   | 5.92 | 190 | 11 |\n",
    "| male   | 5.58 | 170 | 12 |\n",
    "| male   | 5.92 | 165 | 10 |\n",
    "| female | 5    | 100 |  6 |\n",
    "| female | 5.5  | 150 |  8 |\n",
    "| female | 5.42 | 130 |  7 |\n",
    "| female | 5.75 | 150 |  9 |\n",
    "\n",
    "$\n",
    "posterior(M)=\\frac{P(M)\\ p(height|M)\\ p(weight|M)\\ p(foot size|M)}{evidence}\\\\\n",
    "posterior(F)=\\frac{P(F)\\ p(height|F)\\ p(weight|F)\\ p(foot size|F)}{evidence}\n",
    "$\n",
    "\n",
    "풀이:\n",
    "1. 사전확률: P(M), P(F)은 주관적, 객관적으로 구함.\n",
    "    * 사전 경험: 남녀 모두 0.5\n",
    "    * 위 데이터에서 남녀의 확률 (즉 4/8), 이 것도 0.5\n",
    "2. Likelihood\n",
    "    * 확률을 구하기 위해서는 분포를 추정 - 남녀의 키,몸무게,발길이 정규분포로.\n",
    "    * 정규분포 확률을 계산하기 위해 평균, 표준편차 계산\n",
    "3. Evidence\n",
    "    * P(M) p(height|M) p(weight|M) p(foot size|M) + P(F) p(height|F) p(weight|F) p(foot size|F) \n",
    "    * 이 값은 상수 값이므로, 사후확률 계산에서 제외함.\n",
    "4. 분류\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        height(feet)  weight(lbs)  foot size(inches)\n",
      "sex                                                 \n",
      "female        5.4175       132.50               7.50\n",
      "male          5.8550       176.25              11.25\n",
      "\n",
      "[2 rows x 3 columns]\n",
      "        height(feet)  weight(lbs)  foot size(inches)\n",
      "sex                                                 \n",
      "female      0.097225   558.333333           1.666667\n",
      "male        0.035033   122.916667           0.916667\n",
      "\n",
      "[2 rows x 3 columns]\n",
      "        height(feet)  weight(lbs)  foot size(inches)\n",
      "sex                                                 \n",
      "female      0.311809    23.629078           1.290994\n",
      "male        0.187172    11.086779           0.957427\n",
      "\n",
      "[2 rows x 3 columns]\n",
      "Pr(Height|Male)= 1.57888296476\n",
      "사후확률의 분자 값 Male=6.16822078418e-09 Female=0.000537789695219\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. 데이터 준비\n",
    "df=pd.DataFrame([\n",
    "    ['male',6,180,12],\n",
    "    ['male',5.92,190,11],\n",
    "    ['male',5.58,170,12],\n",
    "    ['male',5.92,165,10],\n",
    "    ['female',5,100,6],\n",
    "    ['female',5.5,150,8],\n",
    "    ['female',5.42,130,7],\n",
    "    ['female',5.75,150,9]],\n",
    "    columns=['sex','height(feet)','weight(lbs)','foot size(inches)']\n",
    ")\n",
    "\n",
    "# 2. 사전확률 Prior Probability\n",
    "pr_m=0.5\n",
    "# p(height|M) ~ N(6,남자키평균,남자키표준편차), 즉 N(6,5.855,0.187)\n",
    "pr_f=0.5\n",
    "\n",
    "# 3. Likelihood update\n",
    "# 3.1 정규분포 확률계산에 필요한 평균, 표준편차 계산\n",
    "print df.groupby('sex').mean()\n",
    "#        height(feet)  weight(lbs)  foot size(inches)\n",
    "#female        5.4175       132.50               7.50\n",
    "#male          5.8550       176.25              11.25\n",
    "\n",
    "print df.groupby('sex').var()\n",
    "#        height(feet)  weight(lbs)  foot size(inches)\n",
    "#female      0.097225   558.333333           1.666667\n",
    "#male        0.035033   122.916667           0.916667\n",
    "\n",
    "print df.groupby('sex').std()\n",
    "#        height(feet)  weight(lbs)  foot size(inches)\n",
    "#female      0.311809    23.629078           1.290994\n",
    "#male        0.187172    11.086779           0.957427\n",
    "\n",
    "# 3.2 확률 계산\n",
    "#예제 - Pr(Height=6|Male)\n",
    "mu=5.8550\n",
    "std=0.187172\n",
    "x=6\n",
    "n=float((x-mu)/abs(std))\n",
    "numer=np.exp(-n*n/2)\n",
    "denom=abs(std)*np.sqrt(2*np.pi)\n",
    "print \"Pr(Height|Male)=\",numer/denom #1.579 확률이 아니라 확률분포값\n",
    "\n",
    "# 함수로 만들기\n",
    "def normpdf(x, mu=0, std=1):\n",
    "    n = float(x-mu) / abs(std)\n",
    "    g = np.exp(-n*n/2) / (abs(std) * np.sqrt(2*np.pi))\n",
    "    return g\n",
    "\n",
    "pr_h_m=normpdf(6,5.855,0.187172) #1.5788829647561371\n",
    "pr_f_m=normpdf(8,11.25,0.957) #0.0013050759944537563\n",
    "pr_w_m=normpdf(130,176.25,11.0868) #5.9869297985549439e-06\n",
    "post_m=pr_m * pr_h_m * pr_f_m * pr_w_m #6.1682207841818461e-09\n",
    "\n",
    "pr_w_f=normpdf(130,132.5,23.629)\n",
    "pr_h_f = normpdf(6,5.4175,0.311809)\n",
    "pr_f_f=normpdf(8,7.5,1.291)\n",
    "post_f=pr_f*pr_h_f*pr_w_f*pr_f_f\n",
    "post_f #0.00053778969521895402\n",
    "\n",
    "#4. argmax를 구하면 1, 즉 post_f일 확률이 높으므로, 여성이라고 구분함.\n",
    "print \"사후확률의 분자 값 Male={0} Female={1}\".format(post_m,post_f)\n",
    "print np.argmax([post_m,post_f]) #1 즉 female로 예측.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제\n",
    "\n",
    "* 문서분류 Document classification\n",
    "* word vector - 단어key, 빈도value\n",
    "    * 문장 full stop으로 분리\n",
    "    * 단어 분리 bag of words\n",
    "    * 품사 morph\n",
    "    * tokenizer - regexp whitespace\n",
    "    * 중복 단어 제거\n",
    "    * stopwords\n",
    "* hold-out cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n of voca: 32\n",
      "voca: set(['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my'])\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "my\n",
      "dog\n",
      "has\n",
      "flea\n",
      "problems\n",
      "help\n",
      "please\n",
      "32 [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "word vector:\n",
      "[[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0], [1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]]\n",
      "before p1Num [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "after p1Num [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0.\n",
      "  0.  0.  0.  1.  1.  0.  1.  0.  1.  0.  1.  0.  0.  0.]\n",
      "before p1Num [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0.\n",
      "  0.  0.  0.  1.  1.  0.  1.  0.  1.  0.  1.  0.  0.  0.]\n",
      "after p1Num [ 0.  0.  0.  1.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  1.  0.  1.\n",
      "  0.  1.  0.  1.  1.  0.  1.  0.  2.  0.  1.  0.  0.  0.]\n",
      "before p1Num [ 0.  0.  0.  1.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  1.  0.  1.\n",
      "  0.  1.  0.  1.  1.  0.  1.  0.  2.  0.  1.  0.  0.  0.]\n",
      "after p1Num [ 0.  0.  0.  1.  1.  0.  0.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.\n",
      "  0.  2.  0.  1.  1.  0.  2.  0.  3.  0.  1.  0.  0.  0.]\n",
      "-p0- p0Num=[ 1.  1.  1.  0.  0.  1.  1.  1.  0.  1.  1.  1.  1.  0.  0.  2.  0.  0.\n",
      "  1.  0.  1.  1.  0.  1.  1.  1.  0.  1.  0.  1.  1.  3.]\n",
      "p0Denom=24.0\n",
      "p0Vect[ 0.04166667  0.04166667  0.04166667  0.          0.          0.04166667\n",
      "  0.04166667  0.04166667  0.          0.04166667  0.04166667  0.04166667\n",
      "  0.04166667  0.          0.          0.08333333  0.          0.\n",
      "  0.04166667  0.          0.04166667  0.04166667  0.          0.04166667\n",
      "  0.04166667  0.04166667  0.          0.04166667  0.          0.04166667\n",
      "  0.04166667  0.125     ]\n",
      "-p1- p1Num=[ 0.  0.  0.  1.  1.  0.  0.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.\n",
      "  0.  2.  0.  1.  1.  0.  2.  0.  3.  0.  1.  0.  0.  0.]\n",
      "p1Denom=19.0\n",
      "p1Vect[ 0.          0.          0.          0.05263158  0.05263158  0.          0.\n",
      "  0.          0.05263158  0.05263158  0.          0.          0.\n",
      "  0.05263158  0.05263158  0.05263158  0.05263158  0.05263158  0.\n",
      "  0.10526316  0.          0.05263158  0.05263158  0.          0.10526316\n",
      "  0.          0.15789474  0.          0.05263158  0.          0.          0.        ]\n",
      "[0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "------ -0.69314718056 -0.484813847227\n",
      "------ -0.69314718056 -0.484813847227\n"
     ]
    }
   ],
   "source": [
    "# learn textbook p.67\n",
    "import numpy as np\n",
    "\n",
    "postingList=[['my','dog','has','flea','problems','help','please'],\n",
    "             ['maybe','not','take', 'him','to','dog','park','stupid'],\n",
    "             ['my','dalmation','is','so','cute','I','love','him'],\n",
    "             ['stop','posting','stupid','worthless','garbage'],\n",
    "             ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
    "         ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "classVec = [0,1,0,1,0,1]\n",
    "\n",
    "# crateVocabList\n",
    "# voca=set()\n",
    "vocabSet=set([])\n",
    "for doc in postingList:\n",
    "    # vocaSet = vocaSet.union(set(doc))\n",
    "    vocabSet=vocabSet | set(doc)\n",
    "\n",
    "print \"n of voca: {0}\\nvoca: {1}\\n\".format(len(vocabSet),vocabSet)\n",
    "vocabList=list(vocabSet)\n",
    "\n",
    "# setOfWords2Vec\n",
    "returnVec = [0]*len(vocabList)\n",
    "print returnVec\n",
    "\n",
    "for word in postingList[0]:\n",
    "    if word in vocabList:\n",
    "        print word\n",
    "        returnVec[vocabList.index(word)] = 1\n",
    "    else:\n",
    "        print \"the word: %s is not in my Vocabulary!\" % word\n",
    "print len(returnVec),returnVec\n",
    "\n",
    "# define function setOfWords2Vec\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print \"the word: %s is not in my Vocabulary!\" % word\n",
    "    return returnVec\n",
    "\n",
    "trainMat=[]\n",
    "for postinDoc in postingList:\n",
    "    trainMat.append(setOfWords2Vec(vocabList, postinDoc))\n",
    "print \"word vector:\\n\",trainMat\n",
    "\n",
    "# trainNB0\n",
    "numTrainDocs=len(trainMat) #6\n",
    "numWords=len(trainMat[0]) #32\n",
    "pAbusive = sum(classVec)/float(numTrainDocs) #prior(abusive) 0.5\n",
    "p0Num = np.zeros(numWords)\n",
    "p1Num = np.zeros(numWords)\n",
    "p0Denom = 0.0\n",
    "p1Denom = 0.0\n",
    "\n",
    "for i in range(numTrainDocs):\n",
    "    if classVec[i] == 1:\n",
    "        print \"before p1Num\",p1Num\n",
    "        p1Num += trainMat[i]\n",
    "        p1Denom += sum(trainMat[i])\n",
    "        print \"after p1Num\",p1Num\n",
    "    else:\n",
    "        p0Num += trainMat[i]\n",
    "        p0Denom += sum(trainMat[i])\n",
    "\n",
    "p1Vect = p1Num/p1Denom\n",
    "p0Vect = p0Num/p0Denom\n",
    "print \"-p0- p0Num={0}\\np0Denom={1}\\np0Vect{2}\".format(p0Num, p0Denom,p0Vect)\n",
    "print \"-p1- p1Num={0}\\np1Denom={1}\\np1Vect{2}\".format(p1Num, p1Denom,p1Vect)\n",
    "\n",
    "# classifyNB\n",
    "import math\n",
    "testEntry = ['love', 'my', 'dalmation']\n",
    "thisDoc = np.array(setOfWords2Vec(vocabList, testEntry))\n",
    "print thisDoc\n",
    "\n",
    "p1 = sum(thisDoc * p1Vect) + math.log(pAbusive)\n",
    "p0 = sum(thisDoc * p0Vect) + math.log(1.0 - pAbusive)\n",
    "print \"------\",p1, p0\n",
    "\n",
    "testEntry = ['stupid', 'garbage']\n",
    "thisDoc = np.array(setOfWords2Vec(vocabList, testEntry))\n",
    "print thisDoc\n",
    "p1 = sum(thisDoc * p1Vect) + math.log(pAbusive)\n",
    "p0 = sum(thisDoc * p0Vect) + math.log(1.0 - pAbusive)\n",
    "print \"------\",p1, p0\n",
    "#if p1 > p0:\n",
    "#    print \"class 1\"\n",
    "#else:\n",
    "#    print \"class 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jsl/Code/git/else/machinelearninginaction/Ch04\n",
      "the error rate is:  0.0\n",
      "['codeine', '15mg', 'for', '203', 'visa', 'only', 'codeine', 'methylmorphine', 'narcotic', 'opioid', 'pain', 'reliever', 'have', '15mg', '30mg', 'pills', '15mg', 'for', '203', '15mg', 'for', '385', '15mg', 'for', '562', 'visa', 'only']\n"
     ]
    }
   ],
   "source": [
    "# unzip email.zip\n",
    "import os\n",
    "dir=os.getenv('HOME')+'/Code/git/else/machinelearninginaction/Ch04'\n",
    "os.chdir(dir)\n",
    "print os.getcwd()\n",
    "import bayes\n",
    "bayes.spamTest()\n",
    "i=1 #1 ~ 26\n",
    "wordList = bayes.textParse(open(dir+'/email/spam/%d.txt' % i).read())\n",
    "print wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.0 19.0\n",
      "[ 1.  1.  1.  0.  0.  1.  1.  1.  0.  1.  1.  1.  1.  0.  0.  2.  0.  0.\n",
      "  1.  0.  1.  1.  0.  1.  1.  1.  0.  1.  0.  1.  1.  3.] [ 0.  0.  0.  1.  1.  0.  0.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.\n",
      "  0.  2.  0.  1.  1.  0.  2.  0.  3.  0.  1.  0.  0.  0.]\n",
      "1\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      ">>> p0 p1 [-0.48481385] [-0.69314718]\n"
     ]
    }
   ],
   "source": [
    "# 첫째, 훈련단계 train\n",
    "# 1. Data read\n",
    "# word vector\n",
    "import numpy as np\n",
    "import math\n",
    "docs=[['my','dog','has','flea','problems','help','please'],\n",
    "     ['maybe','not','take', 'him','to','dog','park','stupid'],\n",
    "     ['my','dalmation','is','so','cute','I','love','him'],\n",
    "     ['stop','posting','stupid','worthless','garbage'],\n",
    "     ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
    "     ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "_voca=set()\n",
    "for doc in docs:\n",
    "    _voca=_voca.union(set(doc))\n",
    "voca=dict()\n",
    "for word in _voca:\n",
    "    voca[word]=len(voca)\n",
    "nword=len(voca)#32\n",
    "ndoc=len(docs) #6\n",
    "nword_doc=np.zeros([nword,ndoc]) #교재 trainMat\n",
    "#docs: list\n",
    "#voca: list\n",
    "def set_nword_doc(docs,voca):\n",
    "    for di in xrange(len(docs)):\n",
    "        #print docs[di]\n",
    "        for word in docs[di]:\n",
    "            if word in voca.keys():\n",
    "                #print \" \",voca[word],word\n",
    "                nword_doc[voca[word],di]+=1\n",
    "set_nword_doc(docs,voca)\n",
    "\n",
    "def printM(ndict,ndictBym):\n",
    "    for i,j in ndict.items():\n",
    "        print \"%20s-%s\" % (i,ndictBym[j])\n",
    "#printM(voca,nword_doc)\n",
    "\n",
    "# class labels\n",
    "ndoc_class=np.zeros([ndoc,1])\n",
    "ndoc_class[1]=1\n",
    "ndoc_class[3]=1\n",
    "ndoc_class[5]=1\n",
    "#print ndoc_class.T #array([[ 0.,  1.,  0.,  1.,  0.,  1.]])\n",
    "#nclass=2\n",
    "#nword_class=np.zeros([nword,nclass])\n",
    "\n",
    "# 2. prior = 클래스가 1인 갯수를 구해서 전체갯수로 나누어 구한 확률\n",
    "pAbusive=len(ndoc_class[ndoc_class[:,0]==1])/float(len(ndoc_class[:,0]))\n",
    "\n",
    "# 3. likelihood update\n",
    "# ndoc_class[:,0]==1 #split only class==1\n",
    "nword_doc_0=nword_doc[:,ndoc_class[:,0]==0]\n",
    "nword_doc_1=nword_doc[:,ndoc_class[:,0]==1]\n",
    "p0denom=nword_doc_0.sum() #19\n",
    "p1denom=nword_doc_1.sum() #24\n",
    "p0num=nword_doc_0.sum(axis=1)\n",
    "p1num=nword_doc_1.sum(axis=1)\n",
    "\n",
    "print p0denom, p1denom\n",
    "print p0num, p1num\n",
    "#p0Num=[1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 2 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 3]\n",
    "#p1Num=[0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 2 0 1 1 0 2 0 3 0 1 0 0 0]\n",
    "p0vect=p0num/p0denom #see if this equals to 교재 p.71\n",
    "p1vect=p1num/p1denom\n",
    "\n",
    "# 둘째, 분류단계 classify\n",
    "#1. data read\n",
    "testEntry = ['love', 'my', 'dalmation'] # love:1 my:31 dalmation:111\n",
    "print voca['love'] #1 리스트일 경우는 이렇게 vocalist.index('love')\n",
    "\n",
    "testDoc=np.zeros([nword,1])\n",
    "for word in testEntry:\n",
    "    testDoc[voca[word]]+=1\n",
    "print testDoc.T\n",
    "#[0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
    "\n",
    "# testDoc을 함수로 비교: setOfWords2Vec(vocabList, inputSet):\n",
    "#def setOfWords2Vec(vocabList, inputSet):\n",
    "#    returnVec = [0]*len(vocabList)\n",
    "#    for word in inputSet:\n",
    "#        if word in vocabList:\n",
    "#            returnVec[vocabList.index(word)] = 1\n",
    "#        else: print \"the word: %s is not in my Vocabulary!\" % word\n",
    "#    return returnVec\n",
    "def set_word_doc(doc,voca):\n",
    "    nword=len(voca)\n",
    "    word_doc=np.zeros([nword,1])    \n",
    "    for word in doc:\n",
    "        if word in voca.keys():\n",
    "            #print \" \",voca[word],word\n",
    "            word_doc[voca[word]]+=1\n",
    "    return word_doc\n",
    "testDocUsingFunction=set_word_doc(testEntry,voca)\n",
    "print testDocUsingFunction.T\n",
    "\n",
    "# 2. classify using trained parameters\n",
    "p0=p0vect*testDoc.T #Transpose해서 곱하지 않으면 dot을 구하는 듯!!!\n",
    "p0=p0.sum(axis=1) + math.log(1.0 - pAbusive)\n",
    "\n",
    "p1=p1vect*testDoc.T #Transpose해서 곱하지 않으면 dot을 구하는 듯!!!\n",
    "p1=p1.sum(axis=1) + math.log(pAbusive)\n",
    "\n",
    "print \">>> p0 p1\",p0, p1\n",
    "#교재 p0:-0.484813847227 p1:-0.69314718056\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
